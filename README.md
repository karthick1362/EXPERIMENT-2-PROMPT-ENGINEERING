# EXP-2-PROMPT-ENGINEERING-

## Aim: 
Comparative Analysis of different types of Prompting patterns and explain with Various Test Scenarios

Experiment:
Test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios. 
Analyze the quality, accuracy, and depth of the generated responses.


## Algorithm:
1. Identify different prompt types (broad, basic, refined, few-shot).  
2. Select test scenarios (creative writing, technical explanation, decision-making).  
3. Apply each prompt type to the scenarios and record responses.  
4. Evaluate outputs based on clarity, accuracy, depth, and relevance.  
5. Compare and analyze results.  
## Output
### Example Comparative Test  

#### Scenario 1: Creative Writing  

- **Broad Prompt**: *"Tell me a story."*  
  - Output: Generic, often unfocused narrative. Lacks depth or personalization.  

- **Basic Prompt**: *"Write a short story about a brave child who saves a village."*  
  - Output: Clear, but still simple. Provides a straightforward storyline.  

- **Refined Prompt**: *"Write a 300-word short story in the style of a folktale about a brave 10-year-old child who uses intelligence rather than strength to save a mountain village from danger."*  
  - Output: Richer narrative, clear structure, cultural tone, and more creativity.  

---

#### Scenario 2: Technical Explanation  

- **Broad Prompt**: *"Explain AI."*  
  - Output: Very general, superficial explanation.  

- **Basic Prompt**: *"Explain Artificial Intelligence in simple terms."*  
  - Output: Decent definition with examples, still at a beginner level.  

- **Refined Prompt**: *"Explain Artificial Intelligence as if you are teaching first-year computer science students. Cover history, types (narrow vs. general), and modern applications in less than 300 words."*  
  - Output: Well-structured, domain-specific, concise yet detailed explanation.  

---

#### Scenario 3: Decision-Making  

- **Broad Prompt**: *"Should I buy a laptop?"*  
  - Output: Generic advice without considering context.  

- **Basic Prompt**: *"Should a student buy a laptop for studies?"*  
  - Output: Reasonable, slightly more focused suggestions.  

- **Refined Prompt**: *"As a technology consultant, recommend whether a computer science undergraduate in 2025 should buy a laptop under â‚¹60,000 for programming and AI projects. Include at least 3 key specifications to look for."*  
  - Output: Context-aware, specific, actionable advice with detailed specifications.  

---

### Comparative Analysis  

| Prompt Type         | Clarity | Accuracy | Depth | Relevance |
|----------------------|---------|----------|-------|-----------|
| Broad/Unstructured   | Low     | Low      | Low   | Low       |
| Basic Prompts        | Medium  | Medium   | Medium| Medium    |
| Refined/Structured   | High    | High     | High  | High      |
| Few-Shot Prompts     | High    | High     | High  | High (especially for tasks like translation, coding, or classification) |

---
## Result
Refined and structured prompts produce clearer, more accurate, and context-aware responses compared to broad or basic prompts, proving that effective prompt design greatly improves output quality.  
